{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results, analysis and demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we present and discuss quantitative results obtained by our models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the SentEval results\n",
    "As a prerequisite we show how to read and summarize results from SentEval evaluations. These will be discussed in more detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| baseline-avg | 78.13 | 79.71 |\n",
      "| baseline-zero | 78.27 | 79.64 |\n",
      "| bilstm-avg | 77.92 | 78.46 |\n",
      "| bilstm-zero | 77.73 | 78.47 |\n",
      "| bimaxlstm-avg | 79.52 | 79.99 |\n",
      "| bimaxlstm-zero | 79.63 | 80.41 |\n",
      "| unilstm-avg | 74.01 | 74.76 |\n",
      "| unilstm-zero | 74.48 | 75.11 |\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SENTEVAL_RESULTS_PATH = Path(\"./senteval_results\")\n",
    "\n",
    "def summarize_senteval_results(results_path):\n",
    "    metrics = ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC', 'SICKEntailment']\n",
    "    # Summarize all results available in SENTEVAL_RESULTS_PATH\n",
    "    result_fnames = [fname for fname in os.listdir(results_path) if os.path.isfile(results_path / fname)]\n",
    "\n",
    "    summaries = []\n",
    "    for fname in result_fnames:\n",
    "        with open(results_path / fname, 'rb') as handle:\n",
    "            results = pickle.load(handle)\n",
    "\n",
    "            dev_accs = [results[metric][\"devacc\"] for metric in metrics]\n",
    "\n",
    "            # Calculate 'macro' scores as average of dev accuracies\n",
    "            micro_avg = np.average(dev_accs)\n",
    "\n",
    "            # Calculate 'micro' scores as the average of dev accuracies weighted by sample size\n",
    "            sample_sizes = np.array([results[metric][\"ndev\"] for metric in metrics])\n",
    "            weights = sample_sizes / np.sum(sample_sizes)\n",
    "            macro_avg = np.average(dev_accs, weights=weights)\n",
    "            \n",
    "            summaries += [(fname, micro_avg, macro_avg)]\n",
    "\n",
    "    return summaries\n",
    "\n",
    "summaries = summarize_senteval_results(SENTEVAL_RESULTS_PATH)\n",
    "\n",
    "# print summaries\n",
    "for fname, micro_avg, macro_avg in summaries:\n",
    "    print(f\"| {Path(fname).stem} | {micro_avg:.2f} | {macro_avg:.2f} |\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the results obtained for 4 types of encoders (baseline, unilstm, bilstm, bimaxlstm). For more information on the encoders we refer to `/models/sentence_encoders.py` and their default parameters. \n",
    "\n",
    "As GloVe does not come with a pre-trained embedding for unknown tokens, we initially carried out experiments using the (300-dimensional) zero-vector. However, GloVe is not centered around zero. So, it is not obvious, that the zero-vector is the best options to choose from. An arguably even more neutral embedding is the vector obtained by averaging over all embeddings in the GloVe dataset. \n",
    "\n",
    "To evalaute the impact of this choice, we have tested all models with both of these embeddings for the unknown-token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below summarizes the results our best models have obtained on SNLI and SentEval datasets.\n",
    "\n",
    "| Encoder       | unknown-emb   | Parameters    | Classifier Parameters | SNLI val acc  | SentEval micro    | SentEval macro|\n",
    "| -----------   | -----------   |-----------    | -----------           | -----------   | -----------       | -----------   |\n",
    "| baseline      | zero          | 0             | 0.6 M                 | 73.82         | 78.27             | 79.64         |\n",
    "| unilstm       | zero          | 19.3 M        | 4.2 M                 | 83.65         | 74.48             | 75.11         |\n",
    "| bilstm        | zero          | 38.5 M        | 8.4 M                 | 83.46         | 77.73             | 78.47         |\n",
    "| bimaxlstm     | zero          | 38.5 M        | 8.4 M                 | 86.89         | **79.63**         | **80.41**     |\n",
    "| baseline      | average       | 0             | 0.6 M                 | 74.17         | 78.13             | 79.71         |\n",
    "| unilstm       | average       | 19.3 M        | 4.2 M                 | 83.63         | 74.01             | 74.76         |\n",
    "| bilstm        | average       | 38.5 M        | 8.4 M                 | 83.40         | 77.92             | 78.46         |\n",
    "| bimaxlstm     | average       | 38.5 M        | 8.4 M                 | **86.93**     | 79.52             | 79.99         |\n",
    "\n",
    "The definitions of \"SentEval micro\" and \"SentEval macro\" are defined in line with Table 3 in Conneau et al. (2017):\n",
    "> In this section, we refer to ”micro” and ”macro” averages of development set (dev) results on transfer tasks whose metrics is accuracy: we compute a ”macro” aggregated score that corresponds to the classical average of dev accuracies, and the ”micro” score that is a sum of the dev accuracies, weighted by the number of dev samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a screenshot of the training metrics, which are publicly available at https://wandb.ai/kieron-kretschmar/representations-from-nli\n",
    "\n",
    "<img src=\"images/training_curves.jpg\" width=\"1200\" alt=\"Training curves\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of quantitative results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performances on SNLI \n",
    "On the SNLI dataset, the order of the models from highest to lowest scoring are, regardless of the embedding being used for the unknown-token: bimaxlstm, bilstm, unilstm, baseline. This order also roughly corresponds to the parameter sizes of the models. \n",
    "\n",
    "What is surprising, however, is that our validation accuracies are higher than theirs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performances on SentEval\n",
    "On the SentEval tasks the performances are unexpected throughout. The baseline scores surprisingly high, better than the unilstm and the bilstm. The reason for this is not clear to us, and could be investigated in future work.\n",
    "\n",
    "The best performances, are obtained by the bimaxlstm with zero unknown-embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with results from Table 3 from Conneau et al. (2017)\n",
    "Conneau et al. (2017) have performed similar experiments. Our unilstm refers to their LSTM, and our bilstm corresponds to their BiLSTM-MAX. The other models have not been reproduced.\n",
    "\n",
    "When comparing our results to those of the original authors, there are clear differences. Our validation accuracies for the SNLI dataset are roughly 2% higher for all comparable models, whereas our scores for the SentEval datasets are usually 4-6% lower.\n",
    "\n",
    "We suspect that one way to obtain higher scores on the SentEval task lies in changing the alignment of the vocabulary. We have chosen to align the vocabulary only with the words from the SNLI corpus for which GloVe embeddings are available. We have chosen not to include words outside SNLI that appear in downstream-tasks because this project is about evaluating *general* sentence representations for which the downstream tasks (e.g. SentEval) are unknown. We would expect the performance on SentEval to go up if the test vocabulary was known during training. We suspect this because even though the encoder would never see those exact tokens during training, it might learn useful information from having seen *similar* tokens' embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The choice for the unknown token\n",
    "We could not determine a significant change in performances between choosing the average GloVe embedding or the zero-vector for the unknown-token's embedding. \n",
    "\n",
    "One additional version of the experiment might use a *weighted* average of all GloVe embeddings instead, with weights being determined by the frequency of the corresponding token in e.g. the training corpus. This is left for future work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative results and error analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we present a qualitative discussion of the unilstm (zero) model and its limitations. The model has been trained on the NLI task with `train.py`. We load the pre-trained model and evaluate its predictions on the NLI task, where it is given a premise and a hypothesis and has to predict whether the hypothesis is either entailed by, contradicting or neutral towards the premise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from models import NLIModel\n",
    "from data import SNLIDataModule\n",
    "import nltk\n",
    "\n",
    "# Download nltk prerequisite for tokenization if not available already\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# The checkpoints and vocabulary have to be downloaded seperately, see README.md for further instructions\n",
    "ENCODER_CHECKPOINT_PATH = Path(\"./checkpoints/best/unilstm-zero.ckpt\")\n",
    "CLASSIFIER_CHECKPOINT_PATH = Path(\"./checkpoints/best/unilstm-zero-classifier.ckpt\")\n",
    "VOCABULARY_PATH = Path(\"./cache/vocab-zero.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder from checkpoints\\best\\unilstm-zero.ckpt\n",
      "Encoder loaded.\n",
      "Loading classifier from checkpoints\\best\\unilstm-zero-classifier.ckpt\n",
      "Classifier loaded.\n",
      "Loading vocabulary from cache\\vocab-zero.pkl\n",
      "Vocabulary loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Development\\Miniconda3\\envs\\atcs\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Development\\Miniconda3\\envs\\atcs\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'classifier' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['classifier'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# Load \n",
    "print(f\"Loading encoder from {ENCODER_CHECKPOINT_PATH}\")\n",
    "encoder = torch.load(ENCODER_CHECKPOINT_PATH)\n",
    "encoder.eval()\n",
    "print(f\"Encoder loaded.\")\n",
    "\n",
    "# Load classifier\n",
    "print(f\"Loading classifier from {CLASSIFIER_CHECKPOINT_PATH}\")\n",
    "classifier = torch.load(CLASSIFIER_CHECKPOINT_PATH)\n",
    "classifier.eval()\n",
    "print(f\"Classifier loaded.\")\n",
    "\n",
    "# Load vocab\n",
    "print(f\"Loading vocabulary from {VOCABULARY_PATH}\")\n",
    "with open(VOCABULARY_PATH, 'rb') as handle:\n",
    "    vocab = pickle.load(handle)\n",
    "print(f\"Vocabulary loaded.\")\n",
    "\n",
    "# Create model with full pipeline for NLI inference\n",
    "model = NLIModel(encoder=encoder, classifier=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for doing inference and printing results in a pretty way\n",
    "def predict_example(nli_model, premise, hypothesis, label = \"unspecified\"):\n",
    "    label_meanings = {\n",
    "        0: \"Entailment\",\n",
    "        1: \"Neutral\",\n",
    "        2: \"Contradiction\",\n",
    "    }\n",
    "\n",
    "    # Create batch containing the example in the form expected by the model\n",
    "    premise_tensor = torch.stack([vocab.get_embedding(token) for token in nltk.word_tokenize(premise.lower())], dim=0)\n",
    "    hypothesis_tensor = torch.stack([vocab.get_embedding(token) for token in nltk.word_tokenize(hypothesis.lower())], dim=0)\n",
    "\n",
    "    premise_length = premise_tensor.shape[0]\n",
    "    hypothesis_length = hypothesis_tensor.shape[0]\n",
    "\n",
    "    example_batch = (\n",
    "        (torch.nn.utils.rnn.pad_sequence([premise_tensor], batch_first=True), [premise_length]),\n",
    "        (torch.nn.utils.rnn.pad_sequence([hypothesis_tensor], batch_first=True), [hypothesis_length]),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    logits = nli_model.forward(example_batch)[0]\n",
    "    predicted_class = logits.argmax().item()\n",
    "\n",
    "    print(f\"Premise: \\\"{premise}\\\"\")\n",
    "    print(f\"Hypothesis: \\\"{hypothesis}\\\"\")\n",
    "    print(f\"Correct label: {label}\")\n",
    "    print(f\"The model predicts: {label_meanings[predicted_class]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple example\n",
    "In this section we demonstrate the usage of our models on a simple example. The premise and hypothesis can be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"It is snowing outside my house\"\n",
      "Hypothesis: \"Today is a great day to go swimming at the beach\"\n",
      "Correct label: Contradiction\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"It is snowing outside my house\"\n",
    "hypothesis = \"Today is a great day to go swimming at the beach\"\n",
    "label = \"Contradiction\"\n",
    "predict_example(model, premise, hypothesis, label=label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opposites and negations\n",
    "To establish limitations of the model, we choose more difficult examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"Two men sitting in the sun\"\n",
      "Hypothesis: \"Nobody is sitting in the shade\"\n",
      "Correct label: Neutral\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"Two men sitting in the sun\"\n",
    "hypothesis = \"Nobody is sitting in the shade\"\n",
    "label = \"Neutral\"\n",
    "predict_example(model, premise, hypothesis, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"A man is walking a dog\"\n",
      "Hypothesis: \"No cat is outside\"\n",
      "Correct label: Neutral\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"A man is walking a dog\"\n",
    "hypothesis = \"No cat is outside\"\n",
    "label = \"Neutral\"\n",
    "predict_example(model, premise, hypothesis, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, the hypothesis contains a negated statement about something that sounds opposite of the premise (i.e. \"sun\"<->\"Nobody ... shade\" and \"dog\"<->\"No cat\"). In both cases the model falsely predicts the relationship as a contradiction both relationship. \n",
    "\n",
    "Our intuition is that the model not picking up the negation, but predicting a contradiction because of the opposite-aspect.\n",
    "\n",
    "(On a sidenote, a cognitive bias occuring in humans, \"What You See is All There Is\" (WYSIATI), described by Daniel Kahneman in his book \"Thinking, Fast and Slow\" could also be applied to explain this behaviour. Because if the premise was _all_ there is and nothing else, then the hypothesis would be true in both examples.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of whether our intuition is right, we make 3 types of changes to the examples and observe the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When we remove the negations in the hypotheses, we still observe the same mistake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"Two men sitting in the sun\"\n",
      "Hypothesis: \"Somebody is sitting in the shade\"\n",
      "Correct label: Neutral\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"Two men sitting in the sun\"\n",
    "hypothesis = \"Somebody is sitting in the shade\"\n",
    "label = \"Neutral\"\n",
    "predict_example(model, premise, hypothesis, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"A man is walking a dog\"\n",
      "Hypothesis: \"A cat is outside\"\n",
      "Correct label: Neutral\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"A man is walking a dog\"\n",
    "hypothesis = \"A cat is outside\"\n",
    "label = \"Neutral\"\n",
    "predict_example(model, premise, hypothesis, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When we change the terms in the hypotheses that sound opposite to a term in the premises (\"shade\" for \"sun\" and \"cat\" for \"dog\"), we still observe the same mistake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"Two men sitting in the sun\"\n",
      "Hypothesis: \"Nobody is sitting in a chair\"\n",
      "Correct label: Neutral\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"Two men sitting in the sun\"\n",
    "hypothesis = \"Nobody is sitting in a chair\"\n",
    "label = \"Neutral\"\n",
    "predict_example(model, premise, hypothesis, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"A man is walking a dog\"\n",
      "Hypothesis: \"No tree is outside\"\n",
      "Correct label: Neutral\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"A man is walking a dog\"\n",
    "hypothesis = \"No tree is outside\"\n",
    "label = \"Neutral\"\n",
    "predict_example(model, premise, hypothesis, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. When we apply both adaptations at the same time, the model gets it right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"Two men sitting in the sun\"\n",
      "Hypothesis: \"Somebody is sitting in a chair\"\n",
      "Correct label: Neutral\n",
      "The model predicts: Neutral\n"
     ]
    }
   ],
   "source": [
    "premise = \"Two men sitting in the sun\"\n",
    "hypothesis = \"Somebody is sitting in a chair\"\n",
    "label = \"Neutral\"\n",
    "predict_example(model, premise, hypothesis, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: \"A man is walking a dog\"\n",
      "Hypothesis: \"A cloud is outside\"\n",
      "Correct label: Neutral\n",
      "The model predicts: Neutral\n"
     ]
    }
   ],
   "source": [
    "premise = \"A man is walking a dog\"\n",
    "hypothesis = \"A cloud is outside\"\n",
    "label = \"Neutral\"\n",
    "predict_example(model, premise, hypothesis, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion of qualitative analysis\n",
    "Our conclusion from these examples is that while the model does perform well on simple examples, as further supported by the quantitative results, the model can easily be fooled. We have gathered sporadic evidence that the model is particularly susceptive to giving wrong predictions when negations and opposing words are involved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
