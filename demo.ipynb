{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we show how a model trained with `train.py` on the SNLI dataset performs the Natural Language Inference task on which it was originally trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from models import NLIModel\n",
    "from data import SNLIDataModule\n",
    "import nltk\n",
    "\n",
    "# Download nltk prerequisite for tokenization if not available already\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The checkpoints and vocabulary have to be downloaded seperately, see README.md for further instructions\n",
    "ENCODER_CHECKPOINT_PATH = Path(\"./checkpoints/best/unilstm-zero.ckpt\")\n",
    "CLASSIFIER_CHECKPOINT_PATH = Path(\"./checkpoints/best/unilstm-zero-classifier.ckpt\")\n",
    "VOCABULARY_PATH = Path(\"./cache/vocab-zero.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder from checkpoints\\best\\unilstm-zero.ckpt\n",
      "Encoder loaded.\n",
      "Loading classifier from checkpoints\\best\\unilstm-zero-classifier.ckpt\n",
      "Classifier loaded.\n",
      "Loading vocabulary from cache\\vocab-zero.pkl\n",
      "Vocabulary loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Development\\Miniconda3\\envs\\atcs\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Development\\Miniconda3\\envs\\atcs\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'classifier' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['classifier'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# Load \n",
    "print(f\"Loading encoder from {ENCODER_CHECKPOINT_PATH}\")\n",
    "encoder = torch.load(ENCODER_CHECKPOINT_PATH)\n",
    "encoder.eval()\n",
    "print(f\"Encoder loaded.\")\n",
    "\n",
    "# Load classifier\n",
    "print(f\"Loading classifier from {CLASSIFIER_CHECKPOINT_PATH}\")\n",
    "classifier = torch.load(CLASSIFIER_CHECKPOINT_PATH)\n",
    "classifier.eval()\n",
    "print(f\"Classifier loaded.\")\n",
    "\n",
    "# Load vocab\n",
    "print(f\"Loading vocabulary from {VOCABULARY_PATH}\")\n",
    "with open(VOCABULARY_PATH, 'rb') as handle:\n",
    "    vocab = pickle.load(handle)\n",
    "print(f\"Vocabulary loaded.\")\n",
    "\n",
    "# Create model with full pipeline for NLI inference\n",
    "model = NLIModel(encoder=encoder, classifier=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for doing inference and printing results in a pretty way\n",
    "def predict_example(nli_model, premise, hypothesis):\n",
    "    label_meanings = {\n",
    "        0: \"Entailment\",\n",
    "        1: \"Neutral\",\n",
    "        2: \"Contradiction\",\n",
    "    }\n",
    "    premise = premise.lower()\n",
    "    hypothesis = hypothesis.lower()\n",
    "\n",
    "    # Create batch containing the example in the form expected by the model\n",
    "    premise_tensor = torch.stack([vocab.get_embedding(token) for token in nltk.word_tokenize(premise)], dim=0)\n",
    "    hypothesis_tensor = torch.stack([vocab.get_embedding(token) for token in nltk.word_tokenize(hypothesis)], dim=0)\n",
    "\n",
    "    premise_length = premise_tensor.shape[0]\n",
    "    hypothesis_length = hypothesis_tensor.shape[0]\n",
    "\n",
    "    example_batch = (\n",
    "        (torch.nn.utils.rnn.pad_sequence([premise_tensor], batch_first=True), [premise_length]),\n",
    "        (torch.nn.utils.rnn.pad_sequence([hypothesis_tensor], batch_first=True), [hypothesis_length]),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    logits = nli_model.forward(example_batch)[0]\n",
    "    predicted_class = logits.argmax().item()\n",
    "    print(f\"The model predicts: {label_meanings[predicted_class]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example\n",
    "In this section we demonstrate the usage of our models on a simple example. The premise and hypothesis can of course be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct label: Contradiction\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"It is snowing outside my house\"\n",
    "hypothesis = \"Today is a great day to go swimming at the beach\"\n",
    "print(\"Correct label: Contradiction\")\n",
    "predict_example(model, premise, hypothesis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard examples\n",
    "Let's continue with some hard examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct label: Neutral\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"Two men sitting in the sun\"\n",
    "hypothesis = \"Nobody is sitting in the shade\"\n",
    "print(\"Correct label: Neutral\")\n",
    "predict_example(model, premise, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct label: Neutral\n",
      "The model predicts: Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = \"A man is walking a dog\"\n",
    "hypothesis = \"No cat is outside\"\n",
    "print(\"Correct label: Neutral\")\n",
    "predict_example(model, premise, hypothesis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, the hypothesis contains a statement about something that sounds opposite of the premise (i.e. \"sun\"<->\"shade\" and \"dog\"<->\"cat\"), but the hypothesis is not clearly scoped. The model has difficulty predicting the correct relationship (neutral, in both cases) with certainty. It seems like the model assumes that the premise contains the entire truth. \n",
    "\n",
    "Interestingly, a similar cognitive bias occuring in humans called \"What You See is All There Is\" (WYSIATI) is described by Daniel Kahneman in Thinking, Fast and Slow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
